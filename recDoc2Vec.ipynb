{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cython` not found (But cell magic `%%cython` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython\n",
    "%time\n",
    "%cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# nltk processing\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"recTitleIngredRating.csv\") \n",
    "dfTitle = data['title']\n",
    "dfIngred = data['ingred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(ds):\n",
    "    ds = ds.str.replace('tablespoons', '')\n",
    "    ds = ds.str.replace('tablespoon', '')\n",
    "    ds = ds.str.replace('teaspoon', '')\n",
    "    ds = ds.str.replace('teaspoons', '')\n",
    "    ds = ds.str.replace('cup', '')\n",
    "    ds = ds.str.replace('cups', '')\n",
    "    ds = ds.str.replace('ounce', '')\n",
    "    ds = ds.str.replace('ounces', '')\n",
    "    ds = ds.str.replace('pound', '')\n",
    "    ds = ds.str.replace('pounds', '')\n",
    "    ds = ds.str.replace('pinch', '')\n",
    "    ds = ds.str.replace('pinches', '')\n",
    "    ds = ds.str.replace('taste', '')\n",
    "    ds = ds.str.replace('dash', '')\n",
    "    ds = ds.str.replace('dashes', '')\n",
    "\n",
    "\n",
    "    ds = ds.str.replace('1/8', '')\n",
    "    ds = ds.str.replace('1/4', '')\n",
    "    ds = ds.str.replace('1/3', '')\n",
    "    ds = ds.str.replace('1/2', '')\n",
    "    ds = ds.str.replace('2/3', '')\n",
    "    ds = ds.str.replace('1', '')\n",
    "    ds = ds.str.replace('1', '')\n",
    "    ds = ds.str.replace('2', '')\n",
    "    ds = ds.str.replace('3', '')\n",
    "    ds = ds.str.replace('4', '')\n",
    "    ds = ds.str.replace('5', '')\n",
    "    ds = ds.str.replace('6', '')\n",
    "    ds = ds.str.replace('7', '')\n",
    "    ds = ds.str.replace('8', '')\n",
    "    ds = ds.str.replace('9', '')\n",
    "    ds = ds.str.replace('0', '')\n",
    "    ds = ds.str.replace('(', '')\n",
    "    ds = ds.str.replace(')', '')\n",
    "    ds = ds.str.replace(' s ', '')\n",
    "    ds = ds.str.replace(\"'\", '')\n",
    "    ds = ds.str.replace('.', '')\n",
    "    ds = ds.str.replace(' to ', '')\n",
    "    ds = ds.str.replace('[', '')\n",
    "    ds = ds.str.replace(']', '')\n",
    "    ds = ds.str.replace('\"', '')\n",
    "\n",
    "    ds = ds.str.replace(' or ', '')\n",
    "    ds = ds.str.replace(' as ', '')\n",
    "    ds = ds.str.replace(' u ', '')\n",
    "    return ds\n",
    "    \n",
    "data['ingred'] = clean_data(data['ingred'])\n",
    "data['title'] = clean_data(data['title'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwrds = stopwords.words('english')\n",
    "# aux function to clean up text\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^\\w\\s]',' ', sentence)\n",
    "    sentence = re.sub('_',' ', sentence)\n",
    "    sentence = re.sub('\\d+',' ', sentence)\n",
    "    cleaned = ' '.join([w for w in sentence.split() if not w in stopwrds])\n",
    "    cleaned = ' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    cleaned = ' '.join([w for w in cleaned.split() if not len(w)<=2 ])\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up review docs, add utf8 encoding\n",
    "#review_docs['textClean'] = review_docs.apply(lambda row: cleaning_text(row['text'].encode(\"utf8\")), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a few functions for doc2vec processing\n",
    "def split_sentence(sentence):\n",
    "    words = re.split('\\W+', sentence.lower())\n",
    "    return [word for word in words if word != \"\"]\n",
    "\n",
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        dfList = dfIngred.values.tolist()\n",
    "        for i in range(len(dfList)):\n",
    "            yield doc2vec.TaggedDocument(words=split_sentence(dfIngred[i]), tags=['%s' % dfIngred[i]])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the doc2vec model\n",
    "mydocs = MyDocs()\n",
    "model = doc2vec.Doc2Vec(mydocs, vector_size = 200, window = 8, min_count = 5, workers = 4)\n",
    "model.save(\"title.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lean', 0.6304351687431335), ('minced', 0.6254710555076599), ('crushed', 0.6162852048873901), ('onion', 0.6086111068725586), ('paprika', 0.6044589877128601)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# testing similar words\n",
    "print(model.most_similar(positive=[\"ground\",\"beef\"], negative=[\"slow\"], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"ground beef\".split()\n",
    "\n",
    "new_vector = model.infer_vector(tokens)\n",
    "\n",
    "sims = model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  margarine, softened,   peanut butter,   white sugar,   packed brown sugar,  egg,   vanilla extract, /  baking soda,   salt,  /all-purpose flour',\n",
       "  0.9509450197219849),\n",
       " ('  unsweetened almond milk,   frozen blackberries,   frozen blueberries,   coconut butter,   honey,   chia seeds',\n",
       "  0.9504584074020386),\n",
       " ('  frozen pineapple chunks,   frozen mango chunks,   toasted coconut vanilla yogurt,   milk',\n",
       "  0.950424313545227),\n",
       " (' /all-purpose flour,   baking soda,   baking powder,   butter, softened,  white sugar,  egg,   vanilla extract',\n",
       "  0.9490064382553101),\n",
       " ('  fresh lime juice,   white wine vinegar,   salt,   freshly ground black pepper,   ground ginger,   dried basil,   dried thyme,   dried parsley,   hot pepper sauce,   cayenne pepper,   vegetable oil,  fresh swordfish fillets',\n",
       "  0.9485344886779785),\n",
       " ('  soy sauce,   honey,   distilled white vinegar,   ground ginger,   garlic powder,   vegetable oil,  flank steak',\n",
       "  0.9473416805267334),\n",
       " ('all-purpose flour,   salt, baking powder,   white sugar,  eggs,  warm milk,   butter, melted,   vanilla extract',\n",
       "  0.9451062679290771),\n",
       " (' lean ground beef, tomato-vegetable juice cocktail,    can diced tomatoes,    can kidney beans, drained and rinsed,    can black beans, rinsed and drained,    can whole kernel corn,    packages taco seasoning mix',\n",
       "  0.9439655542373657),\n",
       " ('  milk, white sugar,   unsweetened cocoa powder,   crunchy peanut butter,   butter, rolled oats,   vanilla extract',\n",
       "  0.9439477324485779),\n",
       " ('  olive oil,  medium onion, chopped,  bay leaves,   ground cumin,   dried oregano,   salt,  stalks celery, chopped,  green bell peppers, chopped,  jalapeno peppers, chopped,  cloves garlic, chopped,    cans chopped green chile peppers, drained,    packages vegetarian burger crumbles,    cans whole peeled tomatoes, crushed,   chili powder,   ground black pepper,    can kidney beans, drained,    can garbanzo beans, drained,    can black beans,    can whole kernel corn',\n",
       "  0.9437569975852966)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save a pickle file\n",
    "import pickle\n",
    "\n",
    "with open('models/model_ingred_only.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingred</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wins Shrimp and Spaghetti</td>\n",
       "      <td>uncooked spaghetti,   butter,   Creole-style s...</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheez-It® Fried Tofu Pita Pocket</td>\n",
       "      <td>package extra-firm tofu, cut into -inch cub...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>King Crab Appetizers</td>\n",
       "      <td>packages refrigerated biscuit dough,    pac...</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spinach Basil Pasta Salad</td>\n",
       "      <td>package bow tie pasta,    package spinach l...</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Butternut Squash Kugel I</td>\n",
       "      <td>package frozen butternut squash, cubed,   m...</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0         Wins Shrimp and Spaghetti   \n",
       "1  Cheez-It® Fried Tofu Pita Pocket   \n",
       "2              King Crab Appetizers   \n",
       "3         Spinach Basil Pasta Salad   \n",
       "4          Butternut Squash Kugel I   \n",
       "\n",
       "                                              ingred  rating  \n",
       "0  uncooked spaghetti,   butter,   Creole-style s...    4.41  \n",
       "1     package extra-firm tofu, cut into -inch cub...    4.00  \n",
       "2     packages refrigerated biscuit dough,    pac...    4.37  \n",
       "3     package bow tie pasta,    package spinach l...    4.67  \n",
       "4     package frozen butternut squash, cubed,   m...    4.17  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a unique summary corpus\n",
    "#summary_docs = games[['uniqueID','summary']].append(movies[['uniqueID', 'summary']]).append(tv[['uniqueID','summary']])\n",
    "#summary_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define class for summary Doc2vec\n",
    "\n",
    "# MyDocs reading from a data frame\n",
    "class MyDocs_summary(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(data.shape[0]):\n",
    "            yield doc2vec.TaggedDocument(words=split_sentence(data.iloc[i,0]), tags=['%s' % data.iloc[i,1]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the doc2vec model on summary\n",
    "mydocs_summary = MyDocs_summary()\n",
    "model_summary = doc2vec.Doc2Vec(mydocs_summary, vector_size = 200, window = 8, min_count = 5, workers = 4)\n",
    "model_summary.save(\"summary.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smoothie', 0.9997438192367554), ('zucchini', 0.9997418522834778), ('sauce', 0.9997351169586182), ('shrimp', 0.9997339844703674), ('lemon', 0.9997339248657227)]\n"
     ]
    }
   ],
   "source": [
    "print(model_summary.wv.most_similar(positive=[\"stew\", \"beef\"], negative=[\"slow\"], topn=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining both summary and critic reviews doc2vec :model_summary and model \n",
    "summary_ingred_docvecs = np.zeros(shape=(len(model_summary.docvecs),400))\n",
    "for i in range(len(model_summary.docvecs)):\n",
    "    summary_ingred_docvecs[i] = np.concatenate((model_summary.docvecs[i],model.docvecs[i]), axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pickle file\n",
    "import pickle\n",
    "with open('summary_ingred_docvecs.pickle', 'wb') as handle:\n",
    "    pickle.dump(summary_ingred_docvecs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "####### TEST REC FROM HERE\n",
    "# For the app, the code for the recommendation would load a pickle from this cell\n",
    "\n",
    "#to load a pickle file\n",
    "with open('summary_ingred_docvecs.pickle', 'rb') as f:\n",
    "    summary_ingred_docvecs = pickle.load(f)\n",
    "    \n",
    "# Auxiliary functions for simple recommendation system \n",
    "\n",
    "# Calculate cosine similarity between two vecotrs \n",
    "def cossim(v1, v2): \n",
    "    return np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / np.sqrt(np.dot(v2, v2)) \n",
    "\n",
    "# return top_n values from a list\n",
    "def top_n_index(l,n):\n",
    "    return sorted(range(len(l)), key=lambda i: l[i])[-(n+1):-1] #-1 to take off the own product from the returned index list\n",
    "\n",
    "# return a list of unique_id for a given category[\"games\", \"movies\",\"tv\"]\n",
    "def category_id_range(category_list):\n",
    "    # range of ids for each category\n",
    "    games_range = range(1,20417)\n",
    "    movies_range = range(20417, 25887)\n",
    "    tv_range = range(25887, 27865)\n",
    "    \n",
    "    category_range = []\n",
    "    for i in category_list:\n",
    "        if i==\"games\":\n",
    "            category_range = category_range + games_range\n",
    "        elif i==\"movies\":\n",
    "            category_range = category_range + movies_range\n",
    "        else:\n",
    "            category_range = category_range + tv_range\n",
    "    \n",
    "    return category_range\n",
    "\n",
    "def content_recommend(item_id, top_n, inputs):\n",
    "    input_vec = inputs[item_id - 1]\n",
    "    \n",
    "    #compute similarity array\n",
    "    sim_array = map(lambda v: cossim(input_vec, v), inputs)\n",
    "    \n",
    "    # recommendation's index (set to 50 to get enough to filter out later)\n",
    "    recommendation_index = top_n_index(sim_array, 500)\n",
    "    \n",
    "    # recommendation's unique id\n",
    "    recommendation_unique_id = [i+1 for i in recommendation_index]\n",
    "    \n",
    "    # recommendation's cossim values\n",
    "    recommendation_cossim = [sim_array[i] for i in recommendation_index]\n",
    "    \n",
    "    top_products = zip(recommendation_unique_id, recommendation_cossim)\n",
    "    \n",
    "    # get the range of unique id for a given category prefered by user\n",
    "    #category_range = category_id_range(category_list)\n",
    "    \n",
    "    result = [i for i in top_products]  # if i[0] in category_range]\n",
    "    \n",
    "    return result[-top_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
