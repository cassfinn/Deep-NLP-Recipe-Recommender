{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cython` not found (But cell magic `%%cython` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython\n",
    "%time\n",
    "%cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# nltk processing\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"recTitleCatIngredCats.csv\") \n",
    "dfTitle = data['title']\n",
    "dfIngred = data['ingred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data2(ds):\n",
    "    arr = ['tablespoons','tablespoon','teaspoons','teaspoon','cups','cup','ounces','ounce','pounds','pound','pinches','pinch','tastes','taste','dashes','dash','/',' or ',' as ',' and ',\n",
    "          '-','[',']','(',')','.','slices','sliced','slice', '0','1','2','3','4','5','6','7','8','9','juiced','diced','minced','chopped',' cans',' canned ',' can ', ' inches ', ' inch ',\n",
    "          'package', 'finely', 'crushed', 'shredded', 'divided', ' melted ', ' jar ','thawed', ' cut ', ' to ', ' thick ', ' across ', ' bones ', 'optional', ' split ', ' torn ',\n",
    "          ' into ', ' strips ', 'peeled',' pieces', ' lightly ', ' beaten ','ground black', 'scrubbed', 'jars', 'beaten','pitted','halved', 'thinly','as needed','large','medium','small'\n",
    "          'roughly','fluid','drained','crumbled','rinsed','cubed','seeded','fresh',' plus ', ' more ', 'deboned', 'prepared', 'cubes', 'uncooked', 'cooked', 'grated', ' lean ','into chunks',\n",
    "          'coarsely','seeded','freshly',' freshly', 'fresh', ' with ', 'liquid','quartered','lengthwise','small',' into ',' bite ', ' sized ', ' ly ', ' ed ', ' s ','cut into', ' up ',\n",
    "          'trimmed','frozen','chunks','segments','softened',' cut ','pieces','broken',' rings',' torn ', ' ',\"'\"]\n",
    " \n",
    "    for item in arr:\n",
    "        ds = ds.str.replace(item,'')\n",
    "     \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       TacoMeatFilling:,groundbeef,tacoseasoning,Batt...\n",
       "1       groundbeef,Pace®TacoSeasoningMix,\"Campbells®Co...\n",
       "2       wholewheatbread,groundbeef,greenonion,,onion,s...\n",
       "3       groundbeef,onionpowder,honeymustard,garlicpowd...\n",
       "4       beefliver,milk,,butter,,Vidaliaonions,,allpurp...\n",
       "5       lime,,garlic,driedoregano,groundcumin,chipotle...\n",
       "6       quartwater,carrots,,potatoes,,onion,,salsa,hot...\n",
       "7       groundbeef,porksausage,onion,,egg,,salt,pepper...\n",
       "8       groundbeef,beefbouilloncube,water,clovesgarlic...\n",
       "9       beefstewmeat,water,onion,,tomatoes,mixedvegeta...\n",
       "10      beefeyeofroundroast,koshersalt,garlicpowder,pe...\n",
       "11      groundbeef,Worcestershiresauce,tomatosauce,fri...\n",
       "12      groundbeef,garlicpowder,sausageflavoredspaghet...\n",
       "13      elbowmacaroni,groundbeef,onion,,condensedtomat...\n",
       "14      groundbeef,onion,,clovesgarlic,,basil,driedore...\n",
       "15      groundbeef,onion,celery,condensedvegetablesoup...\n",
       "16      groundbeef,butterflavoredcrackers,onion,,eggs,...\n",
       "17      groundbeef,onion,,celery,garlic,Cheddarcheese,...\n",
       "18      groundbeef,mushroomsstemsremoved,reserved,marg...\n",
       "19      vegetableoil,allpurposeflour,garlicpowder,salt...\n",
       "20      groundbeef,yellowonion,,greenbellpepper,,redbe...\n",
       "21      groundbeef,eggs,drybreadcrumbs,ketchup,monosod...\n",
       "22      applejuice,brownsugar,mustard,cornedbeefbriske...\n",
       "23      bonelessbeefchuckroast,applejuiceconcentrate,,...\n",
       "24      shortening,onions,,groundbeef,Hungariansweetpa...\n",
       "25      groundbeef,allpurposeflour,cubebeefbouillon,sa...\n",
       "26      chilisauce,fishsauce,darksesameoil,gingerroot,...\n",
       "27      hoisinsauce,sherry,soysauce,barbequesauce,gree...\n",
       "28      soysauce,ricewine,brownsugar,cornstarch,vegeta...\n",
       "29      soysauce,sesameoil,brownsugar,clovesgarlic,,re...\n",
       "                              ...                        \n",
       "8080    shrimp,deveined,sourcream,Swisscheese,jumbopas...\n",
       "8081    driedoregano,salt,groundwhitepepper,pepper,cay...\n",
       "8082    butter,lemonjuice,,butter,Dijonmustard,raspber...\n",
       "8083    water,quinoa,oliveoil,redonion,,greenbellpeppe...\n",
       "8084    spinach,filletsrockfish,cherrytomatoes,,vegeta...\n",
       "8085    butter,canolaoil,celerystalk,greenpepper,green...\n",
       "8086    honey,oliveoil,basil,strawberryjam,redpepperfl...\n",
       "8087    avocados,,,tomatoes,,sweetonion,,saladshrimp,s...\n",
       "8088    headromainelettuce,extravirginoliveoil,redwine...\n",
       "8089    filletsredsnapper,garlicpowder,saltpepper,pica...\n",
       "8090    extravirginoliveoil,onion,,clovesgarlic,redpep...\n",
       "8091    deepdishpiecrust,eggs,,milk,mayonnaise,cornsta...\n",
       "8092    linguinepasta,oliveoil,onion,,clovesgarlic,,al...\n",
       "8093    skinlesscentercutsalmonfillet,into,milk,finebr...\n",
       "8094    waterpackedtuna,flaked,mayonnaise,Dijonmustard...\n",
       "8095    eggyolks,,heavycream,buttermargarine,drysherry...\n",
       "8096    butter,onion,,tuna,,mixedvegetables,condensedc...\n",
       "8097    whitevinegar,fishsauce,whitesugar,limejuice,cl...\n",
       "8098    creamcheese,,smokedsalmon,,capers,to,greenonio...\n",
       "8099    honey,limejuice,clovesgarlic,,tilapiafillets,s...\n",
       "8100    solidwhitetunapackedinwater,,mayonnaise,driedd...\n",
       "8101    salmonfillets,lemonjuice,,drieddillweed,lemonp...\n",
       "8102    shrimpdeveined,coconutmilk,water,piecegalangal...\n",
       "8103    greenbellpepper,,onion,,butter,imitationcrabme...\n",
       "8104    butter,allpurposeflour,onions,,clovegarlic,,ha...\n",
       "8105    angelhairpasta,bayshrimp,greenonions,ranchstyl...\n",
       "8106    lasagnanoodles,butter,onion,creamcheese,,cotta...\n",
       "8107    drywhitewine,water,salmonfillets,into,butter,,...\n",
       "8108    filletshaddock,saltpepper,romaplumtomatoes,,re...\n",
       "8109    asparagus,cut,pecans,,headsredleaflettuce,torn...\n",
       "Name: ingred, Length: 8110, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfIngred = clean_data2(data['ingred'])\n",
    "\n",
    "# remove measurements and anything else that is not a food ingredient\n",
    "data['ingred'] = clean_data2(data['ingred'])\n",
    "\n",
    "#data['title'] = clean_data(data['title'])\n",
    "\n",
    "#dfIngred.to_csv('ingredClean.csv', sep=' ')\n",
    "\n",
    "data['ingred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8110, 2738)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "#tfidf_matrix = tfidf.fit_transform(data['ingred'])\n",
    "tfidf_matrix = tfidf.fit_transform(data['title'])\n",
    "\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingred: 5,460 terms were used to describe 8110 records?\n",
    "\n",
    "\n",
    "#With this matrix in hand, you can now compute a similarity score. There are several candidates for this; such as the euclidean, the Pearson and the cosine similarity scores. \n",
    "\n",
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "1-Dish Taco Bake                   0\n",
       "15-Minute Dinner Nachos Supreme    1\n",
       "A Firefighter's Meatloaf           2\n",
       "Aaron's Missouri Burger            3\n",
       "Absolute Best Liver and Onions     4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Construct a reverse map of indices and movie titles\n",
    "#indices = pd.Series(data.index, index=data['ingred']).drop_duplicates()\n",
    "indices = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "\n",
    "indices.head()\n",
    "\n",
    "#def findIngred(ingredient):\n",
    "#    ingredient = 'groundbeef'\n",
    "#    recs = []\n",
    "#    for item in indices:\n",
    "#        for subitem in item:\n",
    "#            if subitem == ingredient:\n",
    "#                recs.append(item)\n",
    "#    return recs\n",
    "\n",
    "#srecs = findIngred('groundbeef')\n",
    "#srecs\n",
    "\n",
    "#indices\n",
    "#indices['beefeyeofroundroast,koshersalt,garlicpowder,pepper']\n",
    "#idx = indices['beefeyeofroundroast,koshersalt,garlicpowder,pepper']\n",
    "#sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#sim_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in ingredient as input and outputs most similar ingredients\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "#    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sorted(sim_scores, reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    recipe_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return data['title'].iloc[recipe_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1-Dish Taco Bake\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(\"Aaron's Missouri Burger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/cassc042/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/cassc042/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2e4e339e3ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstopwrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# aux function to clean up text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleaning_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/cassc042/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwrds = stopwords.words('english')\n",
    "# aux function to clean up text\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^\\w\\s]',' ', sentence)\n",
    "    sentence = re.sub('_',' ', sentence)\n",
    "    sentence = re.sub('\\d+',' ', sentence)\n",
    "    cleaned = ' '.join([w for w in sentence.split() if not w in stopwrds])\n",
    "    cleaned = ' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    cleaned = ' '.join([w for w in cleaned.split() if not len(w)<=2 ])\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up review docs, add utf8 encoding\n",
    "#review_docs['textClean'] = review_docs.apply(lambda row: cleaning_text(row['text'].encode(\"utf8\")), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a few functions for doc2vec processing\n",
    "def split_sentence(sentence):\n",
    "    words = re.split('\\W+', sentence.lower())\n",
    "    return [word for word in words if word != \"\"]\n",
    "\n",
    "# MyDocs reading from a data frame\n",
    "class MyDocs(object):\n",
    "    def __iter__(self):\n",
    "        dfList = dfIngred.values.tolist()\n",
    "        for i in range(len(dfList)):\n",
    "            yield doc2vec.TaggedDocument(words=split_sentence(dfIngred[i]), tags=['%s' % dfIngred[i]])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the doc2vec model\n",
    "mydocs = MyDocs()\n",
    "model = doc2vec.Doc2Vec(mydocs, vector_size = 200, window = 8, min_count = 5, workers = 4)\n",
    "model.save(\"title.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lean', 0.6304351687431335), ('minced', 0.6254710555076599), ('crushed', 0.6162852048873901), ('onion', 0.6086111068725586), ('paprika', 0.6044589877128601)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# testing similar words\n",
    "print(model.most_similar(positive=[\"ground\",\"beef\"], negative=[\"slow\"], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"ground beef\".split()\n",
    "\n",
    "new_vector = model.infer_vector(tokens)\n",
    "\n",
    "sims = model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  margarine, softened,   peanut butter,   white sugar,   packed brown sugar,  egg,   vanilla extract, /  baking soda,   salt,  /all-purpose flour',\n",
       "  0.9509450197219849),\n",
       " ('  unsweetened almond milk,   frozen blackberries,   frozen blueberries,   coconut butter,   honey,   chia seeds',\n",
       "  0.9504584074020386),\n",
       " ('  frozen pineapple chunks,   frozen mango chunks,   toasted coconut vanilla yogurt,   milk',\n",
       "  0.950424313545227),\n",
       " (' /all-purpose flour,   baking soda,   baking powder,   butter, softened,  white sugar,  egg,   vanilla extract',\n",
       "  0.9490064382553101),\n",
       " ('  fresh lime juice,   white wine vinegar,   salt,   freshly ground black pepper,   ground ginger,   dried basil,   dried thyme,   dried parsley,   hot pepper sauce,   cayenne pepper,   vegetable oil,  fresh swordfish fillets',\n",
       "  0.9485344886779785),\n",
       " ('  soy sauce,   honey,   distilled white vinegar,   ground ginger,   garlic powder,   vegetable oil,  flank steak',\n",
       "  0.9473416805267334),\n",
       " ('all-purpose flour,   salt, baking powder,   white sugar,  eggs,  warm milk,   butter, melted,   vanilla extract',\n",
       "  0.9451062679290771),\n",
       " (' lean ground beef, tomato-vegetable juice cocktail,    can diced tomatoes,    can kidney beans, drained and rinsed,    can black beans, rinsed and drained,    can whole kernel corn,    packages taco seasoning mix',\n",
       "  0.9439655542373657),\n",
       " ('  milk, white sugar,   unsweetened cocoa powder,   crunchy peanut butter,   butter, rolled oats,   vanilla extract',\n",
       "  0.9439477324485779),\n",
       " ('  olive oil,  medium onion, chopped,  bay leaves,   ground cumin,   dried oregano,   salt,  stalks celery, chopped,  green bell peppers, chopped,  jalapeno peppers, chopped,  cloves garlic, chopped,    cans chopped green chile peppers, drained,    packages vegetarian burger crumbles,    cans whole peeled tomatoes, crushed,   chili powder,   ground black pepper,    can kidney beans, drained,    can garbanzo beans, drained,    can black beans,    can whole kernel corn',\n",
       "  0.9437569975852966)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save a pickle file\n",
    "import pickle\n",
    "\n",
    "with open('models/model_ingred_only.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingred</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wins Shrimp and Spaghetti</td>\n",
       "      <td>uncooked spaghetti,   butter,   Creole-style s...</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheez-It® Fried Tofu Pita Pocket</td>\n",
       "      <td>package extra-firm tofu, cut into -inch cub...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>King Crab Appetizers</td>\n",
       "      <td>packages refrigerated biscuit dough,    pac...</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spinach Basil Pasta Salad</td>\n",
       "      <td>package bow tie pasta,    package spinach l...</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Butternut Squash Kugel I</td>\n",
       "      <td>package frozen butternut squash, cubed,   m...</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0         Wins Shrimp and Spaghetti   \n",
       "1  Cheez-It® Fried Tofu Pita Pocket   \n",
       "2              King Crab Appetizers   \n",
       "3         Spinach Basil Pasta Salad   \n",
       "4          Butternut Squash Kugel I   \n",
       "\n",
       "                                              ingred  rating  \n",
       "0  uncooked spaghetti,   butter,   Creole-style s...    4.41  \n",
       "1     package extra-firm tofu, cut into -inch cub...    4.00  \n",
       "2     packages refrigerated biscuit dough,    pac...    4.37  \n",
       "3     package bow tie pasta,    package spinach l...    4.67  \n",
       "4     package frozen butternut squash, cubed,   m...    4.17  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a unique summary corpus\n",
    "#summary_docs = games[['uniqueID','summary']].append(movies[['uniqueID', 'summary']]).append(tv[['uniqueID','summary']])\n",
    "#summary_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define class for summary Doc2vec\n",
    "\n",
    "# MyDocs reading from a data frame\n",
    "class MyDocs_summary(object):\n",
    "    def __iter__(self):\n",
    "        for i in range(data.shape[0]):\n",
    "            yield doc2vec.TaggedDocument(words=split_sentence(data.iloc[i,0]), tags=['%s' % data.iloc[i,1]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the doc2vec model on summary\n",
    "mydocs_summary = MyDocs_summary()\n",
    "model_summary = doc2vec.Doc2Vec(mydocs_summary, vector_size = 200, window = 8, min_count = 5, workers = 4)\n",
    "model_summary.save(\"summary.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smoothie', 0.9997438192367554), ('zucchini', 0.9997418522834778), ('sauce', 0.9997351169586182), ('shrimp', 0.9997339844703674), ('lemon', 0.9997339248657227)]\n"
     ]
    }
   ],
   "source": [
    "print(model_summary.wv.most_similar(positive=[\"stew\", \"beef\"], negative=[\"slow\"], topn=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining both summary and critic reviews doc2vec :model_summary and model \n",
    "summary_ingred_docvecs = np.zeros(shape=(len(model_summary.docvecs),400))\n",
    "for i in range(len(model_summary.docvecs)):\n",
    "    summary_ingred_docvecs[i] = np.concatenate((model_summary.docvecs[i],model.docvecs[i]), axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pickle file\n",
    "import pickle\n",
    "with open('summary_ingred_docvecs.pickle', 'wb') as handle:\n",
    "    pickle.dump(summary_ingred_docvecs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "####### TEST REC FROM HERE\n",
    "# For the app, the code for the recommendation would load a pickle from this cell\n",
    "\n",
    "#to load a pickle file\n",
    "with open('summary_ingred_docvecs.pickle', 'rb') as f:\n",
    "    summary_ingred_docvecs = pickle.load(f)\n",
    "    \n",
    "# Auxiliary functions for simple recommendation system \n",
    "\n",
    "# Calculate cosine similarity between two vecotrs \n",
    "def cossim(v1, v2): \n",
    "    return np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / np.sqrt(np.dot(v2, v2)) \n",
    "\n",
    "# return top_n values from a list\n",
    "def top_n_index(l,n):\n",
    "    return sorted(range(len(l)), key=lambda i: l[i])[-(n+1):-1] #-1 to take off the own product from the returned index list\n",
    "\n",
    "# return a list of unique_id for a given category[\"games\", \"movies\",\"tv\"]\n",
    "def category_id_range(category_list):\n",
    "    # range of ids for each category\n",
    "    games_range = range(1,20417)\n",
    "    movies_range = range(20417, 25887)\n",
    "    tv_range = range(25887, 27865)\n",
    "    \n",
    "    category_range = []\n",
    "    for i in category_list:\n",
    "        if i==\"games\":\n",
    "            category_range = category_range + games_range\n",
    "        elif i==\"movies\":\n",
    "            category_range = category_range + movies_range\n",
    "        else:\n",
    "            category_range = category_range + tv_range\n",
    "    \n",
    "    return category_range\n",
    "\n",
    "def content_recommend(item_id, top_n, inputs):\n",
    "    input_vec = inputs[item_id - 1]\n",
    "    \n",
    "    #compute similarity array\n",
    "    sim_array = map(lambda v: cossim(input_vec, v), inputs)\n",
    "    \n",
    "    # recommendation's index (set to 50 to get enough to filter out later)\n",
    "    recommendation_index = top_n_index(sim_array, 500)\n",
    "    \n",
    "    # recommendation's unique id\n",
    "    recommendation_unique_id = [i+1 for i in recommendation_index]\n",
    "    \n",
    "    # recommendation's cossim values\n",
    "    recommendation_cossim = [sim_array[i] for i in recommendation_index]\n",
    "    \n",
    "    top_products = zip(recommendation_unique_id, recommendation_cossim)\n",
    "    \n",
    "    # get the range of unique id for a given category prefered by user\n",
    "    #category_range = category_id_range(category_list)\n",
    "    \n",
    "    result = [i for i in top_products]  # if i[0] in category_range]\n",
    "    \n",
    "    return result[-top_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
